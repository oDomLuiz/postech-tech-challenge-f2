# Tech Challenge - Fase 2: Pipeline Batch Bovespa | Ingest√£o e Arquitetura de Dados

Este projeto √© o entreg√°vel principal da **Fase 2** da P√≥s-Gradua√ß√£o em **Machine Learning Engineering**. O objetivo √© demonstrar habilidades em engenharia de dados, arquitetura cloud e processamento em larga escala, criando um pipeline completo para ingest√£o e transforma√ß√£o de dados da B3 (Bolsa de Valores Brasileira).

O desafio consiste em desenvolver um pipeline ETL (Extract, Transform, Load) que:
1. **Extrai** dados da B3 via web scraping do IBOV (√çndice Bovespa)
2. **Transforma** os dados em formato otimizado para an√°lise
3. **Carrega** os dados em um data lake na AWS S3

---

## üèóÔ∏è Arquitetura do Projeto

O pipeline de dados foi estruturado em tr√™s etapas principais:

### 1. **Ingest√£o (Web Scraping) - `src/scraping.py`**
Um script Python automatizado que navega pelo site `https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBOV`. Utiliza Selenium para fazer web scraping din√¢mico dos dados do √≠ndice Bovespa, incluindo:
- C√≥digo de negocia√ß√£o
- Nome da a√ß√£o
- Tipo de ativo
- Quantidade te√≥rica
- Participa√ß√£o percentual no √≠ndice

Os dados s√£o salvos localmente em formato **Parquet particionado por data** (`b3_data/ano_mes_dia=YYYY-MM-DD/`) e posteriormente enviados para o **AWS S3** (bucket `fiap-luiz-mlet`).

### 2. **Transforma√ß√£o (AWS Glue ETL) - `src/etl_glue.py`**
Um job do AWS Glue que realiza transforma√ß√µes sofisticadas nos dados:
- L√™ dados da parti√ß√£o mais recente do S3
- Renomeia colunas para padr√£o snake_case
- Converte tipos de dados (strings para n√∫meros)
- Adiciona metadados (data de refer√™ncia, dias desde √∫ltima atualiza√ß√£o)
- Agrega dados por a√ß√£o com totaliza√ß√µes
- Salva dados refinados em S3 no formato Parquet particionado

### 3. **Orquestra√ß√£o (AWS Lambda) - `src/lambda_function.py`**
Uma fun√ß√£o Lambda que atua como orquestradora do pipeline:
- Monitora novos arquivos carregados no S3 (via S3 events)
- Dispara automaticamente o job Glue quando novos dados chegam
- Fornece logging e monitoramento do status de execu√ß√£o

---

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem:** Python 3.9+
* **Web Scraping:** `Selenium` e `WebDriver Manager`
* **Manipula√ß√£o de Dados:** `Pandas` e `PyArrow`
* **Cloud (AWS):** 
  - S3 (Data Lake)
  - Glue (ETL)
  - Lambda (Orquestra√ß√£o)
* **Armazenamento:** `Parquet` (formato otimizado)
* **Autentica√ß√£o:** `python-dotenv` (vari√°veis de ambiente) 

---

## ‚öôÔ∏è Instala√ß√£o e Configura√ß√£o

### Pr√©-requisitos
- Python 3.9+
- Chrome/Chromium instalado (para Selenium)
- Conta AWS com credenciais configuradas
- Vari√°veis de ambiente AWS (Access Key, Secret Key, Session Token)

### Passo 1: Clone o Reposit√≥rio
```bash
git clone <URL_DO_REPOSITORIO>
cd postech-tech-challenge-f2
```

### Passo 2: Crie e Ative o Ambiente Virtual
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/Mac
python -m venv .venv
source .venv/bin/activate
```

### Passo 3: Instale as Depend√™ncias
```bash
pip install -r requirements.txt
```

### Passo 4: Configure as Vari√°veis de Ambiente
Crie um arquivo `.env` na raiz do projeto com suas credenciais AWS:
```env
ACCESS_KEY=sua_access_key_aqui
SECRET_KEY=sua_secret_key_aqui
SESSION_TOKEN=seu_session_token_aqui
```

---

## üöÄ Execu√ß√£o

### Op√ß√£o 1: Executar Scraping Manualmente
```bash
# Windows (via batch script)
run_scraping.bat

# Linux/Mac
python src/scraping.py
```

### Op√ß√£o 2: Agendar Scraping (Windows)
O arquivo `run_scraping.bat` pode ser agendado via **Agendador de Tarefas do Windows**:
1. Abrir Agendador de Tarefas
2. Criar nova tarefa
3. Agendar para executar `run_scraping.bat` em hor√°rios espec√≠ficos
4. Script ativa automaticamente o venv e executa o scraping

### Op√ß√£o 3: Pipeline Completo (AWS)
1. **Carregar o job Glue no AWS:**
   - Upload do arquivo `src/etl_glue.py` para AWS Glue
   - Configurar job com role IAM apropriado
   - Usar PySpark engine

2. **Configurar Lambda para orquestra√ß√£o:**
   - Deploy de `src/lambda_function.py`
   - Criar trigger S3 para detectar novos dados
   - Lambda dispara automaticamente o job Glue

---

## üìä Fluxo de Dados

```
B3 Website (IBOV)
       ‚Üì
[Selenium Scraping] ‚Üí parquet files (local)
       ‚Üì
AWS S3 (raw/b3_data/) - Particionado por data
       ‚Üì
[AWS Lambda Trigger]
       ‚Üì
[AWS Glue ETL Job] - Transforma√ß√£o de dados
       ‚Üì
AWS S3 (refined/b3_data/) - Dados processados
```

---

## üìÇ Estrutura de Arquivos

```
postech-tech-challenge-f2/
‚îú‚îÄ‚îÄ README.md                 # Este arquivo
‚îú‚îÄ‚îÄ requirements.txt          # Depend√™ncias Python
‚îú‚îÄ‚îÄ run_scraping.bat          # Script batch para Windows
‚îú‚îÄ‚îÄ .env.example              # Template de vari√°veis de ambiente
‚îú‚îÄ‚îÄ .venv/                    # Ambiente virtual
‚îú‚îÄ‚îÄ b3_data/                  # Dados locais particionados por data
‚îÇ   ‚îú‚îÄ‚îÄ ano_mes_dia=2026-01-10/
‚îÇ   ‚îú‚îÄ‚îÄ ano_mes_dia=2026-01-11/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ scraping.py           # Web scraping da B3
    ‚îú‚îÄ‚îÄ etl_glue.py           # ETL no AWS Glue
    ‚îî‚îÄ‚îÄ lambda_function.py    # Orquestra√ß√£o Lambda
```

---

## üîß Classes e Funcionalidades

### `B3Scraper` (src/scraping.py)
- **Responsabilidade:** Web scraping do IBOV na B3
- **M√©todos principais:**
  - `setup_driver()` - Configura Chrome headless
  - `get_all_pages_data()` - Navega e coleta dados de todas as p√°ginas
  - `scrape()` - Executa o scraping completo

### `DataProcessor` (src/scraping.py)
- **Responsabilidade:** Processamento local dos dados
- **M√©todos principais:**
  - `add_date_column()` - Adiciona coluna de data de ingest√£o
  - `save_to_parquet()` - Salva em formato Parquet particionado

### `S3Uploader` (src/scraping.py)
- **Responsabilidade:** Upload para AWS S3
- **M√©todos principais:**
  - `upload_files()` - Envia arquivos para data lake

### `GlueETL` (src/etl_glue.py)
- **Responsabilidade:** Transforma√ß√£o de dados em escala
- **M√©todos principais:**
  - `fetch_data()` - L√™ dados do S3
  - `rename_columns()` - Normaliza nomes de colunas
  - `cast_columns()` - Converte tipos de dados
  - `aggregate_data()` - Agrega dados por a√ß√£o
  - `run_etl()` - Executa pipeline completo

---

## üìà Dados Processados

### Colunas da B3 (Brutos)
| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| C√≥digo | String | C√≥digo de negocia√ß√£o da a√ß√£o |
| A√ß√£o | String | Nome da a√ß√£o |
| Tipo | String | Tipo de ativo |
| Qtde. Te√≥rica | String | Quantidade te√≥rica no √≠ndice |
| Part. (%) | String | Participa√ß√£o percentual |

### Colunas Processadas (Refinadas)
| Coluna | Tipo | Descri√ß√£o |
|--------|------|-----------|
| codigo | String | C√≥digo normalizado |
| acao | String | Nome da a√ß√£o |
| tipo | String | Tipo de ativo |
| quantidade_teorica | Long | Quantidade te√≥rica (num√©rica) |
| participacao_percentual | Float | Participa√ß√£o em % (num√©rica) |
| quantidade_teorica_total | Long | Total de quantidade te√≥rica |
| quantidade_teorica_acao | Long | Total por a√ß√£o |
| participacao_percentual_acao | Float | Participa√ß√£o agregada por a√ß√£o |
| quantidade_dias_ultima_data_referencia | Int | Dias desde √∫ltima atualiza√ß√£o |
| data_referencia | Date | Data de refer√™ncia |


---

## üîç Monitoramento e Troubleshooting

### Problemas Comuns

#### 1. Erro ao conectar no Chrome
**Erro:** `chromedriver version mismatch`
**Solu√ß√£o:** O `webdriver-manager` baixa automaticamente a vers√£o correta. Verifique se o Chrome est√° instalado.

#### 2. Erro ao fazer upload para S3
**Erro:** `NoCredentialsError` ou `InvalidAccessKeyId`
**Solu√ß√£o:** Verifique se o arquivo `.env` tem as credenciais AWS corretas e se a sess√£o n√£o expirou.

#### 3. Job Glue falhando
**Erro:** `Error when reading data from S3`
**Solu√ß√£o:** Verifique permiss√µes IAM do role Glue, confirme que os dados foram efetivamente enviados para S3.

---

## üìù Logging

Todos os scripts geram logs detalhados com timestamps:
- **N√≠vel de informa√ß√£o:** Opera√ß√µes completadas com sucesso
- **N√≠vel de alerta:** Diret√≥rios n√£o encontrados, arquivos vazios
- **N√≠vel de erro:** Falhas de conex√£o, convers√£o de dados inv√°lida

Logs s√£o exibidos no console e podem ser redirecionados para arquivos.

---

## ü§ù Contribuindo

Para contribuir com melhorias:
1. Crie uma branch para sua feature (`git checkout -b feature/melhoria`)
2. Commit suas mudan√ßas (`git commit -m 'Adiciona melhoria'`)
3. Push para a branch (`git push origin feature/melhoria`)
4. Abra um Pull Request

---

## üìö Documenta√ß√£o Adicional

- [AWS Glue Documentation](https://docs.aws.amazon.com/glue/)
- [Selenium Documentation](https://www.selenium.dev/documentation/)
- [PyArrow / Parquet Documentation](https://arrow.apache.org/docs/python/)
- [B3 - Bovespa](https://www.b3.com.br/)

---

## üë®‚Äçüíª Autor

**Projeto:** Tech Challenge - Fase 2  
**P√≥s-Gradua√ß√£o:** Machine Learning Engineering - FIAP  
**Data:** Janeiro 2026

---

## üìÑ Licen√ßa

Este projeto √© parte do curr√≠culo da FIAP e segue a pol√≠tica institucional de uso.

````